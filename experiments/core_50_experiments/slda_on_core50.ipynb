{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7df44b-739b-464a-a6c2-95806c5eb29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Optional, Sequence\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import lightly\n",
    "from lightly.models.modules import BarlowTwinsProjectionHead\n",
    "from lightly.loss import BarlowTwinsLoss\n",
    "\n",
    "from avalanche.training.plugins import SupervisedPlugin\n",
    "from avalanche.training.templates.supervised import SupervisedTemplate\n",
    "from avalanche.training.plugins.evaluation import default_evaluator\n",
    "from avalanche.models.dynamic_modules import MultiTaskModule\n",
    "from avalanche.models import FeatureExtractorBackbone\n",
    "from avalanche.benchmarks.generators import nc_benchmark, ni_benchmark\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import (\n",
    "    accuracy_metrics,\n",
    "    loss_metrics,\n",
    "    forgetting_metrics,\n",
    "    confusion_matrix_metrics\n",
    ")\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "\n",
    "from datasets.coil100 import Coil100Dataset\n",
    "from self_supervision.ssl import BarlowTwins\n",
    "from clp.clp import CLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56607d47-051b-445c-9395-dec50254441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7200/7200 [00:00<00:00, 585387.86it/s]\n",
      "100%|██████████| 7200/7200 [00:00<00:00, 597727.55it/s]\n",
      "/home/ehajizad/ss_learning/ssl_env/lib/python3.8/site-packages/avalanche/training/plugins/evaluation.py:81: UserWarning: No benchmark provided to the evaluation plugin. Metrics may be computed on inconsistent portion of streams, use at your own risk.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2737587/3652477879.py:128: UserWarning: The Deep SLDA example is not perfectly aligned with the paper implementation since it does not use a base initialization phase and instead starts streming from pre-trained weights.\n",
      "  warnings.warn(\n",
      "/home/ehajizad/ss_learning/ssl_env/lib/python3.8/site-packages/avalanche/training/plugins/evaluation.py:228: UserWarning: Evaluation stream is not equal to the complete test stream. This may result in inconsistent metrics. Use at your own risk.\n",
      "  warnings.warn(msgw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n"
     ]
    }
   ],
   "source": [
    "# # Copyright (c) 2021 ContinualAI.                                              #\n",
    "# Copyrights licensed under the MIT License.                                   #\n",
    "# See the accompanying LICENSE file for terms.                                 #\n",
    "#                                                                              #\n",
    "# Date: 06-04-2021                                                             #\n",
    "# Author(s): Tyler Hayes                                                       #\n",
    "# E-mail: contact@continualai.org                                              #\n",
    "# Website: avalanche.continualai.org                                           #\n",
    "################################################################################\n",
    "\n",
    "\"\"\"\n",
    "This is a simple example on how to use the Deep SLDA strategy.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import warnings\n",
    "from torchvision import transforms\n",
    "\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import (\n",
    "    accuracy_metrics,\n",
    "    loss_metrics,\n",
    "    forgetting_metrics,\n",
    ")\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.benchmarks.classic import CORe50\n",
    "from avalanche.training.supervised.deep_slda import StreamingLDA\n",
    "from avalanche.models import SLDAResNetModel\n",
    "\n",
    "n_classes = 100\n",
    "batch_size = 512\n",
    "shrinkage = 1e-4\n",
    "plastic_cov = True\n",
    "\n",
    "# Device config\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device \", device)\n",
    "\n",
    "root_dir = '/home/ehajizad/ss_learning/neuromorphic-continual-learning'\n",
    "dataset_dir = '/home/ehajizad/ss_learning/ssl_tests/datasets/coil-100'\n",
    "\n",
    "test_size = 0.9\n",
    "train_size = 1 - test_size\n",
    "\n",
    "train_ds = Coil100Dataset(root_dir=dataset_dir, n_classes = n_classes,\n",
    "                          transform=transforms.ToTensor(), size=64,\n",
    "                          train=True, test_size=test_size)\n",
    "test_ds = Coil100Dataset(root_dir=dataset_dir, n_classes = n_classes,\n",
    "                         transform=transforms.ToTensor(), size=64,\n",
    "                         train=False, test_size=test_size)\n",
    "\n",
    "\n",
    "\n",
    "# Load pretrained model\n",
    "resnet = lightly.models.ResNetGenerator('resnet-9')\n",
    "model = nn.Sequential(*list(resnet.children())[:-1],\n",
    "                         nn.AdaptiveAvgPool2d(2))\n",
    "\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\n",
    "        root_dir+\"/models/coil100_simsiam_resnet9.pth\",\n",
    "        map_location=device))\n",
    "\n",
    "# model = model.backbone[0:-1]\n",
    "model = nn.Sequential(*list(model), nn.Flatten())\n",
    "model.eval()\n",
    "\n",
    "feature_size = 2048\n",
    "# feature_size = 32768\n",
    "\n",
    "\n",
    "# ---------\n",
    "\n",
    "# --- TRANSFORMATIONS\n",
    "# _mu = [0.485, 0.456, 0.406]  # imagenet normalization\n",
    "# _std = [0.229, 0.224, 0.225]\n",
    "# transform = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=_mu, std=_std),\n",
    "#     ]\n",
    "# )\n",
    "# ---------\n",
    "\n",
    "# --- BENCHMARK CREATION\n",
    "benchmark = nc_benchmark(train_ds, test_ds, n_experiences=n_classes, \n",
    "                         shuffle=True,task_labels=False\n",
    ")\n",
    "# ---------\n",
    "\n",
    "log_dir = \"logs/clp100/\" + \"conv_slda_test_90\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(epoch=True, experience=True, stream=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=benchmark.n_classes, save_image=False, stream=True),\n",
    "    loggers=[TensorboardLogger(log_dir)],\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# model = SLDAResNetModel(\n",
    "#     device=device,\n",
    "#     arch=\"resnet18\",\n",
    "#     imagenet_pretrained=model,\n",
    "# )\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = StreamingLDA(\n",
    "    model,\n",
    "    criterion,\n",
    "    feature_size,\n",
    "    n_classes,\n",
    "    eval_mb_size=batch_size,\n",
    "    train_mb_size=batch_size,\n",
    "    train_epochs=1,\n",
    "    shrinkage_param=shrinkage,\n",
    "    streaming_update_sigma=plastic_cov,\n",
    "    device=device,\n",
    "    evaluator=eval_plugin,\n",
    ")\n",
    "\n",
    "warnings.warn(\n",
    "    \"The Deep SLDA example is not perfectly aligned with \"\n",
    "    \"the paper implementation since it does not use a base \"\n",
    "    \"initialization phase and instead starts streming from \"\n",
    "    \"pre-trained weights.\"\n",
    ")\n",
    "\n",
    "# TRAINING LOOP\n",
    "print(\"Starting experiment...\")\n",
    "for i, exp in enumerate(benchmark.train_stream):\n",
    "\n",
    "    # fit SLDA model to batch (one sample at a time)\n",
    "    cl_strategy.train(exp)\n",
    "\n",
    "    # evaluate model on test data\n",
    "    cl_strategy.eval(benchmark.test_stream[:i+1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa071419-89b5-4e0b-a76c-9ea61f5ec372",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cl_strategy.eval(benchmark.test_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49e28de6-82d8-4fb4-baa0-21074a4de6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('slda.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "        \n",
    "# with open('clp_cosine.pkl', 'rb') as f:\n",
    "#     results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ddd1c86-0688-4305-9fc1-90245318256b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ConfusionMatrix_Stream/eval_phase/test_stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m16\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConfusionMatrix_Stream/eval_phase/test_stream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(cm)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ConfusionMatrix_Stream/eval_phase/test_stream'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "cm = results[\"ConfusionMatrix_Stream/eval_phase/test_stream\"]\n",
    "plt.imshow(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1009265-ca0e-487a-9c93-6b8e23b01ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[0]\n",
      "[8]\n",
      "[1]\n",
      "[3]\n",
      "[7]\n",
      "[4]\n",
      "[9]\n",
      "[5]\n",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "for i, exp in enumerate(benchmark.train_stream):\n",
    "    print(exp.classes_in_this_experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "467e7396-992c-4465-88f6-d86e8cd56284",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62413b1e-bd5b-4baa-8bd4-16daf6a92d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 2053948), started 9:24:53 ago. (Use '!kill 2053948' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-56b864b3274af911\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-56b864b3274af911\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6009;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir 'logs/clp100/' --port 6009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb892286-e11b-4a5c-a1c8-5c85b6baec02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
